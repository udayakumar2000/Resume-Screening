{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <H3> Functions </H3> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import docx\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pyresparser\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to join all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatinae all resumes to one single dataframe\n",
    "\n",
    "def resume_datasets(dir_path, name):\n",
    "    \n",
    "    # reading all csv files in the directory\n",
    "    df_list = [pd.read_csv(os.path.join(dir_path, filename)) \n",
    "               for filename in os.listdir(dir_path) \n",
    "               if filename.endswith(\".csv\")]\n",
    "    \n",
    "    if len(df_list) > 0:\n",
    "        # concatinating all scv files to one single file\n",
    "        result = pd.concat(df_list, ignore_index=True)\n",
    "        result.to_csv(name + \".csv\", index=False)\n",
    "        print(f\"Concatenated {len(df_list)} CSV files to {name}.csv\")\n",
    "    else:\n",
    "        print(\"No CSV files found in directory.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to clean the resume using regular expression\n",
    "\n",
    "def clean_resume(text):\n",
    "\n",
    "    text = str(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuations\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    #replacing words with numbers\n",
    "    #Eg: I have one year of experience -> I have 1 year of experience\n",
    "    text = re.sub(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\\b', '', text, flags=re.IGNORECASE) \n",
    "\n",
    "    #removing stop words\n",
    "    tokens = nltk.word_tokenize(text.lower()) #tokenize and convert to lower case\n",
    "    tokens = [word for word in tokens if word not in stop_words] \n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean text from skills, degrees, designation\n",
    "\n",
    "def clean_text(skills):\n",
    "\n",
    "  skills = str(skills).replace('[','').replace(']','').replace(\"'\",'')\n",
    "  skills = skills.replace(' ','').lower()\n",
    "  skills = skills.replace(',',' ')\n",
    "\n",
    "  return skills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract information from resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pyresparser library to extract features from resume\n",
    "\n",
    "def extract_info(text):\n",
    "    # creating a new Word document\n",
    "    doc = docx.Document()\n",
    "    doc.add_paragraph(text)\n",
    "    doc.save(\"temp.docx\")\n",
    "    \n",
    "    # using PyResparser to extract information from the resume\n",
    "    extracted_info = pyresparser.ResumeParser(\"temp.docx\").get_extracted_data()\n",
    "\n",
    "    # extracting name, email and skills from the extracted information\n",
    "    name = extracted_info['name']\n",
    "    email = extracted_info['email']\n",
    "    skills = extracted_info['skills']\n",
    "\n",
    "    return name, email, skills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to show wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating wordcloud on resume skills\n",
    "# genarting wordcloud with bigrams\n",
    "\n",
    "def generate_wordcloud(text, n):\n",
    "    # generating n-grams\n",
    "    n_grams = ngrams(text.split(), n)\n",
    "    freq_dict = Counter([' '.join(n_gram) for n_gram in n_grams])\n",
    "    \n",
    "    # creating wordcloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=10,\n",
    "                      stopwords=STOPWORDS)\n",
    "    wordcloud.generate_from_frequencies(freq_dict)\n",
    "\n",
    "    # displaying wordcloud\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
